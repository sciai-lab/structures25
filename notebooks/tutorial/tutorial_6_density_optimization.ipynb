{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1be328",
   "metadata": {},
   "source": [
    "# Tutorial 6: Density Optimization\n",
    "\n",
    "In this notebook, you will learn about the density optimization (denop) method.\n",
    "In more detail, the density will converge towards the groundstate as enough iterations are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffeece",
   "metadata": {},
   "source": [
    "## 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rich\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# this makes sure that code changes are reflected without restarting the notebook\n",
    "# this can be helpful if you want to play around with the code in the repo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mldft.ml.models.mldft_module import MLDFTLitModule\n",
    "\n",
    "# omegaconf is used for configuration management\n",
    "# omegaconf custom resolvers are small functions used in the config files like \"get_len\" to get lengths of lists\n",
    "from mldft.utils import omegaconf_resolvers  # this registers omegaconf custom resolvers\n",
    "from mldft.utils.log_utils.config_in_tensorboard import dict_to_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd682dd",
   "metadata": {},
   "source": [
    "## 1 (Config) settings for denop\n",
    "\n",
    "The main denisty optimization block is applied after the model is trained. \n",
    "In the command line, it could be execute by the following command: \n",
    "\n",
    "```CUDA_VISIBLE_DEVICES=6 python mldft/ofdft/run_density_optimization.py run_path=\"/export/scratch/ialgroup/dft_str25/models/train/runs/088__from_checkpoint_009__str25\\qm9_tf\"  n_molecules=10 split=test```\n",
    "\n",
    "However, it can also be used during training to improve the model's performance. \n",
    "For the following notebook, we will use a pretrained model, which can be accessefd by a checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a708b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a small dataset from huggingface that contains QM9 and QMugs data\n",
    "# and change the DFT_DATA environment variable to the directory where the data is stored\n",
    "\n",
    "# https://huggingface.co/docs/datasets/cache#cache-directory\n",
    "# The default cache directory is `~/.cache/huggingface/datasets`\n",
    "# You can change it by setting this variable to any path you like\n",
    "CACHE_DIR = None  # e.g. change it to \"./hf_cache\"\n",
    "\n",
    "REPO_ID = \"sciai-lab/minimal_data_QM9_QMugs\"\n",
    "\n",
    "print(\"Using tiny datasets\")\n",
    "\n",
    "# clone the full repo\n",
    "# https://huggingface.co/sciai-lab/structures25/tree/main\n",
    "\n",
    "os.environ[\n",
    "    \"HF_HUB_DISABLE_PROGRESS_BARS\"\n",
    "] = \"1\"  # to avoid problems with the progress bar in some environments\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "data_path = snapshot_download(\n",
    "    repo_id=\"sciai-lab/minimal_data_QM9_QMugs\", cache_dir=CACHE_DIR, repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "dft_data = os.environ.get(\"DFT_DATA\", None)\n",
    "os.environ[\"DFT_DATA\"] = data_path\n",
    "print(\n",
    "    f\"Environment variable DFT_DATA has been changed from {dft_data} to {os.environ['DFT_DATA']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "# load the model from the checkpoint (downloaded from our huggingface model repo):\n",
    "\n",
    "# https://huggingface.co/docs/datasets/cache#cache-directory\n",
    "# The default cache directory is `~/.cache/huggingface/datasets`\n",
    "# You can change it by setting this variable to any path you like\n",
    "CACHE_DIR = None  # e.g. change it to \"./hf_cache\"\n",
    "\n",
    "# https://huggingface.co/sciai-lab/structures25/tree/main\n",
    "print(\"Using QM9 model\")\n",
    "qm9_model_path = hf_hub_download(\n",
    "    repo_id=\"sciai-lab/structures25\",\n",
    "    filename=\"trained-on-qm9/trained-on-qm9.ckpt\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _safe_map_location():\n",
    "    tls = torch._utils._thread_local_state\n",
    "    had_attr = hasattr(tls, \"map_location\")\n",
    "    if not had_attr:\n",
    "        setattr(tls, \"map_location\", None)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if hasattr(tls, \"map_location\") and not had_attr:\n",
    "            delattr(tls, \"map_location\")\n",
    "\n",
    "\n",
    "def safe_load_from_ckpt(path):\n",
    "    for attempt in (1, 2):  # retry once, because the first call can trip the bug\n",
    "        try:\n",
    "            with _safe_map_location():\n",
    "                return MLDFTLitModule.load_from_checkpoint(path, map_location=\"cpu\")\n",
    "        except AttributeError as e:\n",
    "            if \"map_location\" in str(e) and attempt == 1:\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "\n",
    "mldft_module_trained = safe_load_from_ckpt(qm9_model_path)\n",
    "mldft_module_trained.eval()  # set model to eval mode\n",
    "\n",
    "print(\"Successfully loaded trained model from checkpoint:\", type(mldft_module_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4428484",
   "metadata": {},
   "source": [
    "For this model, we have to use the \"local_frames_global_natrep_add_lframe\" transformation, which we can access by overwriting the default.\n",
    "\n",
    "But careful with the dataset statistics! The dataset_statistics used to create the SAD intitial guess is the starting point for denop. In the denop we, find two dataset statistics, one in the the model and one in the data section. They differ by the transformations applied. The initial SAD guess is built completely without any transforms applied and is only later transformed using sample.transformation_matrix shortly before starting the denop. To ensure no transforms are applied we use use the \"config_denop.model.dataset_statistics and a specifically called omegaconf custom resolver (\"to_no_basis_transforms_dataset_statistics\") handels the rest.\n",
    "\n",
    "With the \"datamodule.test_dataloader()\", we prepare a test set for density optimization. \n",
    "\n",
    "Additionally, we want to batch the data and for demonstartion purposes only use the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import open_dict\n",
    "\n",
    "from mldft.ml.data.components.convert_transforms import PrepareForDensityOptimization\n",
    "from mldft.ofdft.functional_factory import requires_grid\n",
    "from mldft.utils.omegaconf_resolvers import to_no_basis_transforms_dataset_statistics\n",
    "\n",
    "# for this model we need to use local frames so we need slightly different transforms\n",
    "with initialize(version_base=None, config_path=\"../../configs/ml\"):\n",
    "    config_denop = compose(\n",
    "        config_name=\"train.yaml\",\n",
    "        overrides=[\n",
    "            \"data/transforms=local_frames_global_natrep_add_lframes\",\n",
    "            \"data.dataset_name=QM9_perturbed_fock\",  # this will no longer be necessary once the \"fixed\" is removed from the dataset_name\n",
    "            \"data.transforms.use_cached_data=False\",  # to use untransformed data paths\n",
    "        ],\n",
    "    )\n",
    "    # IMPORTANT: for denop we need to instantiate the model.dataset_statistics, the basis_info and the datamodule\n",
    "    # use open_dict envrionment to modify the config since it is frozen by default\n",
    "    with open_dict(config_denop):\n",
    "        config_denop.model.dataset_statistics = config_denop.data.dataset_statistics.copy()\n",
    "        config_denop.model.dataset_statistics.path = to_no_basis_transforms_dataset_statistics(\n",
    "            dataset_statistics_path=config_denop.model.dataset_statistics.path,\n",
    "            transformation_name=config_denop.data.transforms.name,\n",
    "        )\n",
    "\n",
    "model_dataset_statistics_for_denop = instantiate(config_denop.model.dataset_statistics)\n",
    "\n",
    "# remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config_denop.paths.output_dir = \"example_path\"\n",
    "\n",
    "datamodule = instantiate(config_denop.data.datamodule)\n",
    "datamodule.setup(stage=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4747581",
   "metadata": {},
   "source": [
    "Next, some transformations to the right data type have to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135d31c",
   "metadata": {},
   "source": [
    "Additionally, the denop settings and the density optimizer settings in the config need to be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c654ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ofdft config:\n",
    "with initialize(version_base=None, config_path=\"../../configs/ofdft\"):\n",
    "    config_ofdft = compose(\n",
    "        config_name=\"ofdft.yaml\",\n",
    "    )\n",
    "\n",
    "\n",
    "# print configs for denop:\n",
    "print(\"\\nConfig for density_optimizer:\")\n",
    "rich.print(dict_to_tree(config_ofdft.optimizer, guide_style=\"dim\"))\n",
    "\n",
    "# for denop our model needs the following additional things:\n",
    "# denop_settings = instantiate(config_denop.model.denop_settings)\n",
    "density_optimizer = instantiate(config_ofdft.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14991228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.dataset import OFDataset\n",
    "\n",
    "# set pytorch dtype default to float64 for better numerical accuracy in denop\n",
    "from mldft.ml.data.components.of_data import Representation\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# customise the transformations for density optimization:\n",
    "basis_info = instantiate(config_denop.data.basis_info)\n",
    "transforms = instantiate(config_denop.data.transforms)\n",
    "add_grid = requires_grid(\n",
    "    config_denop.data.target_key, config_ofdft.negative_integrated_density_penalty_weight\n",
    ")\n",
    "transforms.pre_transforms.insert(0, PrepareForDensityOptimization(basis_info, add_grid=add_grid))\n",
    "transforms.add_transformation_matrix = True\n",
    "transforms.use_cached_data = False\n",
    "\n",
    "dataset_kwargs = instantiate(config_denop.data.datamodule.dataset_kwargs)\n",
    "dataset_kwargs.update(\n",
    "    {\n",
    "        \"limit_scf_iterations\": -1,\n",
    "        \"additional_keys_at_ground_state\": {\n",
    "            \"of_labels/energies/e_electron\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_ext\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_hartree\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_kin\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_kin_plus_xc\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_kin_minus_apbe\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_kinapbe\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_xc\": Representation.SCALAR,\n",
    "            \"of_labels/energies/e_tot\": Representation.SCALAR,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "denop_dataset = OFDataset(\n",
    "    paths=datamodule.test_set.paths,\n",
    "    num_scf_iterations_per_path=None,\n",
    "    basis_info=basis_info,\n",
    "    transforms=transforms,\n",
    "    **dataset_kwargs,\n",
    ")\n",
    "\n",
    "sample_double = denop_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.convert_transforms import ToTorch\n",
    "\n",
    "# by default for denop we use double precision\n",
    "print(\"sample.pos.dtype before ToTorch:\", sample_double.pos.dtype)\n",
    "to_float_32 = ToTorch(float_dtype=torch.float32)\n",
    "sample_float = sample_double.clone()\n",
    "sample_float = to_float_32(sample_float)\n",
    "print(\"sample.pos.dtype after ToTorch:\", sample_float.pos.dtype)\n",
    "\n",
    "# after converting to float we can do a simple forward pass through the trained model\n",
    "forward_out_trained = mldft_module_trained.forward(sample_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81da78",
   "metadata": {},
   "source": [
    "## 2 Functional Factory and SAD guess\n",
    "\n",
    "Now, the config settings are prepared and instantiated and we can take a look at the \"FunctionalFactory\" and the Sad guess, which will be calles during the actual denop procedure.\n",
    "\n",
    "The \"FunctionalFactory\" is used to create an energy functional from the trained model that can be used for density optimization.\n",
    "\n",
    "The contributions returned are therefore: ```contributions = [mldft_module_trained, \"hartree\", \"nuclear_attraction\"]```\n",
    "* With our model, we predict T_s + E_xc, i.e. the non-interacting kinetic energy plus the exchange-correlation energy.\n",
    "* The hartree energy is the classical electron-electron repulsion energy based on the current density.\n",
    "* The nuclear attraction energy is the attraction of the electrons to the nuclei based on the current density.\n",
    "\n",
    "--\n",
    "* Also note: Since the nuclear repulsion energy does not depend on the density, it is not part of the functional\n",
    "but is computed later on directly from the mol object via nuclear_repulsion = mol.energy_nuc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional factory (which is a slighly more complicated thing)\n",
    "from mldft.ofdft.functional_factory import FunctionalFactory\n",
    "\n",
    "func_factory = FunctionalFactory.from_module(\n",
    "    module=mldft_module_trained,\n",
    "    xc_functional=config_ofdft.xc_functional,  # not used in our case since we predict T_s + E_xc\n",
    "    negative_integrated_density_penalty_weight=config_ofdft.negative_integrated_density_penalty_weight,\n",
    "    # the latter is zero by default (no penalty for regions with negative electron densities)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89a091",
   "metadata": {},
   "source": [
    "Now, on to the SAD (Sum of Atomic Denisties):\n",
    "\n",
    "The SAD guess is a sum of independent atom-type specific densities that are based on dataset statistics\n",
    "and for which the total number of electrons matches the total number of electrons in the molecule.\n",
    "Even though there are also other first guess methods like MINAO or HÃœCKEL (well established initial guesses already implemented in the `pyscf` package) or the option to learn the initial guess, we use the simple SAD guess as a default in STRUCTURES25 since it is cheapest of the ones listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96346595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ofdft.callbacks import ConvergenceCallback\n",
    "from mldft.ofdft.density_optimization import density_optimization_with_label\n",
    "from mldft.utils.sad_guesser import SADNormalizationMode\n",
    "\n",
    "# since we do use SAD (Sum of Atomic Densities) as initial density guess, we have to specify\n",
    "# the following keyword arguments that are passed to the SAD guesser,\n",
    "# see SADGuesser class for details:\n",
    "\n",
    "sad_guess_kwargs = dict(\n",
    "    dataset_statistics=model_dataset_statistics_for_denop,\n",
    "    normalization_mode=SADNormalizationMode.PER_ATOM_WEIGHTED,\n",
    "    basis_info=basis_info,\n",
    "    weigher_key=\"ground_state_only\",\n",
    "    spherical_average=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb736c5",
   "metadata": {},
   "source": [
    "## 3 Denop process and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change max number of interations:\n",
    "density_optimizer.max_cycle = 10  # You migth want to change the number of iterations to speed things up, but this migth not guarantee convergence\n",
    "\n",
    "metric_dict, callback, energies_label, energy_functional = density_optimization_with_label(\n",
    "    sample=sample_double,  # OFData sample object containing required tensors for the functional.\n",
    "    mol=sample_double.mol,  # Molecule object used for the initial guess and building the grid (used for eval of XC functional).\n",
    "    optimizer=density_optimizer,  # Optimizer used for the density optimization process.\n",
    "    func_factory=func_factory,  # see above\n",
    "    callback=ConvergenceCallback(),  # specifies which iteration to report as the converged result\n",
    "    # (in our case of \"last_iter\" as convergence criterion, this is simply the last iteration.)\n",
    "    initial_guess_str=config_ofdft.initialization,  # in our case SAD is used as initial guess (see above)\n",
    "    max_xc_memory=config_ofdft.ofdft_kwargs.max_xc_memory,  # XC is computed on the grid this defines an upper limit for the grid size\n",
    "    # not relevant when using e_kin_plus_xc as training target\n",
    "    # best doc string explanation is the following:\n",
    "    #  Guess of the maximum memory that should be taken by the aos in MB. Total usage might be higher.\n",
    "    #       Defaults to the pyscf default of 4000MB\n",
    "    normalize_initial_guess=config_ofdft.ofdft_kwargs.normalize_initial_guess,  # Whether to normalize the initial guess to the correct number of electrons.\n",
    "    proj_minao_module=None,  # Lightning module used to improve the initial guess from SAD to some learned initial guess.\n",
    "    sad_guess_kwargs=sad_guess_kwargs,  # see above\n",
    "    convergence_criterion=config_ofdft.convergence_criterion,  # in our case \"last_iter\", i.e. we simply take the last iteration and stop iterating if the gradient norm is below the convergence_tolerance\n",
    "    disable_printing=False,  # Whether to disable printing of the optimization progress.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e5c84",
   "metadata": {},
   "source": [
    "From the \"density_optimization_with_label\", the following properties are returned:\n",
    "* metric_dict: Dictionary containing various metrics collected during the optimization process.\n",
    "* callback: ConvergenceCallback object used to determine convergence.\n",
    "* energies_label: Dictionary containing the energies computed during the optimization process, including the label energies.\n",
    "* energy_functional: The energy functional used for the optimization.\n",
    "\n",
    "If you wnat to you can also look athe results individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a944853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains metrics evaluating the final density and energy after denop (gs=ground_state)\n",
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ofdft.energies import Energies\n",
    "\n",
    "# contains the different energies at the \"converged\" state\n",
    "energies_label.energies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2745c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The callback contains the states of all iterations\n",
    "print(\"Length of callback.states:\", len(callback.energy))\n",
    "# that converged result can be obtained via:\n",
    "print(\"Converged result:\")\n",
    "callback.get_convergence_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94626789",
   "metadata": {},
   "source": [
    "## 4 Visualizations of results\n",
    "\n",
    "As a last step, let's visualize the density optimization process and see how it converges toward the tolance set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce88518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how the gradient norm evolved during denop:\n",
    "plt.plot(callback.gradient_norm, label=\"Gradient norm\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Gradient norm\")\n",
    "plt.title(\"Density optimization convergence\")\n",
    "plt.axhline(\n",
    "    density_optimizer.convergence_tolerance, color=\"red\", linestyle=\"--\", label=\"Tolerance\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ad361",
   "metadata": {},
   "source": [
    "Furthermore, we want to look at the Energy evolvance per iteration (remember we predict T_s + E_xc, and \"hartee\" and \"nuclear_attractio\"n energy  are ajusted acording to the density or independent of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ofdft.ofstate import OFState\n",
    "\n",
    "# OFState returned by the ConvergenceCallback,\n",
    "# amongst other things, it contains the predicted coefficients and energies\n",
    "# at every iteration\n",
    "energies_dict = {key: [] for key in energies_label.energies_dict.keys()}\n",
    "for key in energies_label.energies_dict.keys():\n",
    "    for energy in callback.energy:\n",
    "        energies_dict[key].append(energy.energies_dict[key])\n",
    "    energies_dict[key] = np.array(energies_dict[key])\n",
    "\n",
    "# build the total energy from the different contributions\n",
    "total_energy = np.zeros_like(next(iter(energies_dict.values())))\n",
    "for key in energies_dict.keys():\n",
    "    total_energy += energies_dict[key]\n",
    "energies_dict[\"total_energy\"] = total_energy\n",
    "\n",
    "# plot a curve of how the energy (as predicted by our model) evolved during denop\n",
    "# kin_plus_xc is the energy that our model directly predicts\n",
    "# all other energy contributions are computed from the density (with existing functionals)\n",
    "# the total energy is the sum of all contributions\n",
    "# important note: the total energy is therefore an approximation to the true DFT energy\n",
    "# based on our learned functional (model)\n",
    "for key in energies_dict.keys():\n",
    "    plt.plot(energies_dict[key], label=key)\n",
    "plt.xlabel(\"Denop iteration\")\n",
    "plt.ylabel(\"Predicted energy (mHa)\")\n",
    "plt.title(\"Energy during density optimization\")\n",
    "plt.legend(loc=(1.01, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324daad",
   "metadata": {},
   "source": [
    "Lastly, a full overview to the denop process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddebce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay in the above plot we don't see much change in the energies during denop\n",
    "# more interesting is to look at the difference between the predicted energies\n",
    "# and the energy ground state labels\n",
    "# for that, we can use the plot from plot_density_optimization\n",
    "# that is also shown in the pdf of denop plots:\n",
    "from mldft.ml.data.components.basis_transforms import transform_tensor_with_sample\n",
    "from mldft.ml.data.components.of_data import Representation\n",
    "from mldft.utils.plotting.density_optimization import plot_density_optimization\n",
    "\n",
    "# for comparison, transform the ground state coeffs back to the untransformed representation,\n",
    "# since the trajectory coeffs are transformed back before in the callback\n",
    "gs_coeffs = transform_tensor_with_sample(\n",
    "    sample_double, sample_double.ground_state_coeffs, Representation.VECTOR, invert=True\n",
    ")\n",
    "\n",
    "fig = plot_density_optimization(\n",
    "    callback=callback,\n",
    "    energies_label=energies_label,\n",
    "    coeffs_label=gs_coeffs,  # ground state density coefficients as label used for computing the density error\n",
    "    sample=sample_double,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd640b57",
   "metadata": {},
   "source": [
    "In this plot, we first see the energy differences to the ground state energy in mHa. In more detail, \n",
    "it oscialltes around the groundstate energy until it converges to a final energy that is slightly above the ground state energy.\n",
    "\n",
    "The second panel shows the error between the predicted and the target density as well as the gradient norm.\n",
    "The density error is computed as the L2 norm of the difference between the predicted and target density on the grid.\n",
    "The gradient norm is the norm of the gradient of the energy with respect to the density coefficients. \n",
    "\n",
    "The third panel shows the change in the density coefficients during denop.\n",
    "\n",
    "Finally, the last panel shows the dipole moment differences to the ground state dipole moment in au (atomic units).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef612c",
   "metadata": {},
   "source": [
    "In this tutorial, we have illustrated some of the inner workings behind density optimization. For many of the small steps like getting a data sample one which we can run density optimization or \n",
    "there exists some high level functionality in our code base to do them (in the [run_density_optimization.py](../../mldft/ofdft/run_density_optimization.py)):  \n",
    "\n",
    "`SampleGenerator`: a class to obain individual data samples from a full model/data config  \n",
    "`run_singlepoint_ofdft`: a function that runs a full density opitmization for the given molecule  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
