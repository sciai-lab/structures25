{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1be328",
   "metadata": {},
   "source": [
    "# Tutorial 5: MLDFT lit module\n",
    "\n",
    "In this Section, we will dive into the model structure and explain how pytorch lightning is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23647f88",
   "metadata": {},
   "source": [
    "## 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rich\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from mldft.utils.log_utils.config_in_tensorboard import dict_to_tree\n",
    "\n",
    "# this makes sure that code changes are reflected without restarting the notebook\n",
    "# this can be helpful if you want to play around with the code in the repo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# omegaconf is used for configuration management\n",
    "# omegaconf custom resolvers are small functions used in the config files like \"get_len\" to get lengths of lists\n",
    "from mldft.utils import omegaconf_resolvers  # this registers omegaconf custom resolvers\n",
    "\n",
    "# download a small dataset from huggingface that contains QM9 and QMugs data (possibly already downloaded)\n",
    "# and change the DFT_DATA environment variable to the directory where the data is stored\n",
    "\n",
    "# https://huggingface.co/docs/datasets/cache#cache-directory\n",
    "# The default cache directory is `~/.cache/huggingface/datasets`\n",
    "# You can change it by setting this variable to any path you like\n",
    "CACHE_DIR = None  # e.g. change it to \"./hf_cache\"\n",
    "\n",
    "# clone the full repo\n",
    "# https://huggingface.co/sciai-lab/structures25/tree/main\n",
    "os.environ[\n",
    "    \"HF_HUB_DISABLE_PROGRESS_BARS\"\n",
    "] = \"1\"  # to avoid problems with the progress bar in some environments\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "data_path = snapshot_download(\n",
    "    repo_id=\"sciai-lab/minimal_data_QM9_QMugs\", cache_dir=CACHE_DIR, repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "dft_data = os.environ.get(\"DFT_DATA\", None)\n",
    "os.environ[\"DFT_DATA\"] = data_path\n",
    "print(\n",
    "    f\"Environment variable DFT_DATA has been changed from {dft_data} to {os.environ['DFT_DATA']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60127dec",
   "metadata": {},
   "source": [
    "## 1 Config and data loading\n",
    "\n",
    "As a first step, we have to load the \"train.yaml\" config as a OmegaConf Dict config. For now, we don't use any overwrites, but just use the default setting for data, optimizer, transforms, basis set, etc. As the [hydra \"tree\" structure](.notebooks/tutorial_4_hydra_omegaconf.ipynb) is used, this already handles the communication and combination of the different config files, e.g. for data and the model.\n",
    "\n",
    "After the data is loaded, we focus for demonstration purposes on one individual sample molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "from mldft.utils.molecules import build_molecule_ofdata\n",
    "\n",
    "# the following initialize already handles the communication and combination\n",
    "# of the different config files, e.g. for data and the model\n",
    "with initialize(version_base=None, config_path=\"../../configs/ml\"):\n",
    "    config = compose(\n",
    "        config_name=\"train.yaml\",\n",
    "        overrides=[\n",
    "            # we need one simple override here but otherwise we just use the default setting (see tutorial 4 for more information)\n",
    "            \"data.dataset_name=QM9_perturbed_fock\",  # this will no longer be necessary once the \"fixed\" is removed from the dataset_name\n",
    "            # Add trainer overrides for demonstration purposes\n",
    "            \"trainer.max_epochs=1\",\n",
    "            \"+trainer.limit_train_batches=1\",\n",
    "            \"+trainer.limit_val_batches=1\",\n",
    "            \"+trainer.enable_checkpointing=False\",\n",
    "            \"data.datamodule.num_workers=0\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config.paths.output_dir = \"example_path\"\n",
    "\n",
    "datamodule = instantiate(config.data.datamodule)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "datamodule.batch_size = 4  # set batch size to 4 (relatively small) for demonstration purposes\n",
    "train_loader = datamodule.train_dataloader()\n",
    "\n",
    "sample = datamodule.train_set[0]\n",
    "\n",
    "# need basis info to build a pySCF molecule object\n",
    "# see below for more details on basis_info\n",
    "basis_info = instantiate(config.data.basis_info)\n",
    "\n",
    "# build a pySCF molecule object from the OFData sample\n",
    "mol = build_molecule_ofdata(sample, basis=basis_info.basis_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57a243",
   "metadata": {},
   "source": [
    "Next, we want to take a look at the machine learning model used to predict\n",
    "the kinetic energy (and possibly other energies) from a given electron density.\n",
    "The main module which handles the training is the MLDFTLitModule.\n",
    "\n",
    "For this, let's take a look at the part of the config that is used to configure the model.\n",
    "It is a very long and nested config, which specifies everything needed for training.\n",
    "\n",
    "You will find in there amongst other things:\n",
    "* The optimizer used to update the model parameters during training.\n",
    "* The learning rate scheduler used to adjust the learning rate after every epoch during training.\n",
    "* The loss function used to compute the training loss: It is used for backpropagation\n",
    "to compute the gradients of the model parameters which will be applied to update each parameter.\n",
    "* The net which is the main neural network architecture that takes the batched sample as input\n",
    "and outputs a prediction for the energy.\n",
    "* The basis_info which specifies the basis set used to represent the density.\n",
    "* The dataset_statistics used to standardize the input densities and the output energy labels to\n",
    "improve and stabilize training.\n",
    "* The density_optimizer and denop_settings which specify how density optimization is performed with a trained model.\n",
    "\n",
    "Question: Can you find the optimizer and the learning rate that we use for training?\n",
    "Question: Can you also find the optimizer and learning rate that we use during density optimization (denop)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from mldft.ml.models.mldft_module import MLDFTLitModule\n",
    "\n",
    "rich.print(dict_to_tree(config.model, guide_style=\"dim\"))\n",
    "\n",
    "# The getattribute function prints a message whenever a hook method is called\n",
    "# Therfore, we can later see in the output which hooks are called during training\n",
    "# (e.g., on_train_start, training_step, etc.)\n",
    "# find more information in the output after the trainer is called\n",
    "\n",
    "\n",
    "def getattribute(self, name):\n",
    "    attr = object.__getattribute__(self, name)\n",
    "    hook_prefixes = (\"on_\", \"training_\", \"validation_\", \"test_\", \"predict_\")\n",
    "    if callable(attr) and any(name.startswith(p) for p in hook_prefixes):\n",
    "\n",
    "        @functools.wraps(attr)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            print(f\"Our lightning module is now calling: {name}\")\n",
    "            return attr(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "    return attr\n",
    "\n",
    "\n",
    "mldft_module = instantiate(config.model)\n",
    "mldft_module.__class__.__getattribute__ = getattribute\n",
    "# the MLDFTLitModule inherits from pl.LightningModule\n",
    "# which is a PyTorch Lightning specific class that handles the training loop\n",
    "print(\"Successfully instantiated model:\", type(mldft_module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3a691",
   "metadata": {},
   "source": [
    "# 2 Forward pass through model\n",
    "\n",
    "Now, let's do a forward pass through the model with one batch of data. \n",
    "\n",
    "The forward output consists of three parts:\n",
    "* First, the predicted energy for the given input electron density (in our case kinetic energy + XC energy).\n",
    "* Second, the predicted gradients of the energy with respect to the input density coefficients.These are computed via automatic differentiation (autodiff) in PyTorch (see example below)\n",
    "* Third, a direct prediction of the ground state density coefficients (or rather the difference between the input density coeffs and the ground state density coeffs).\n",
    "The latter, we usually don't use during training, see coefficient_loss has weight 0.0 in the config above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a forward pass through the model with one batch of data\n",
    "# this is taking some time because the model was not moved to the GPU:\n",
    "print(\"mldft_module.device:\", mldft_module.device, \"\\n\")\n",
    "batch = next(iter(train_loader))\n",
    "forward_out = mldft_module.forward(batch)  # which does the same as mldft_module(batch)\n",
    "\n",
    "print(\"Model output:\", forward_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2365a",
   "metadata": {},
   "source": [
    "## 3 Training step\n",
    "\n",
    "This was a single forward pass through the model, but does not yet look much like training\n",
    "instead we can make a training_step with the model. \n",
    "\n",
    "In more detail, during each training step, the following happens:\n",
    "1. The training loop calls the `training_step` method of the `MLDFTLitModule`.\n",
    "2. Inside `training_step`, the model processes the input batch to produce predictions.\n",
    "3. The loss function computes the loss by comparing the predictions to the true labels.\n",
    "4. Additional training metrics are computed and logged.\n",
    "5. The compuatational graph is saved for a backward pass.\n",
    "\n",
    "Afterwards, the optimizer uses the loss to perform backpropagation and update the model weights.\n",
    "\n",
    "For more information on the optimizer, the Appendix 3 in this notebook can be recommended.\n",
    "\n",
    "To do the training step, we will need a trainer attached to the model.\n",
    "(By the way, the model which we have just loaded is untrained, so the loss will be very large.)\n",
    "\n",
    "**Command for classical training:**\n",
    "\n",
    "Usually you would start a training with a command similar to this one:  \n",
    "```CUDA_VISIBLE_DEVICES=2 python mldft/ml/train.py  experiment=str25/qm9_tf```\n",
    "\n",
    "Quick note: With```CUDA_VISIBLE_DEVICES```, you select which GPU to run the job on. Please, check after accessing the server which GPU is currently free with the following command: ```gpustat```.\n",
    "\n",
    "With the rest of the command you call the main training script with experiment specific config options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer\n",
    "# for that, remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config.paths.output_dir = \"example_path\"\n",
    "config.paths.work_dir = \"example_path\"\n",
    "trainer = instantiate(config.trainer)\n",
    "print(\"Successfully instantiated trainer:\", type(trainer))\n",
    "mldft_module.trainer = trainer  # add the trainer to the module\n",
    "\n",
    "# also, let us disable the logging for this tutorial:\n",
    "mldft_module.log = lambda *args, **kwargs: None\n",
    "mldft_module.log_dict = lambda *args, **kwargs: None\n",
    "\n",
    "train_step_out = mldft_module.training_step(batch)\n",
    "print(\"Output of training step:\", train_step_out.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea2ed3",
   "metadata": {},
   "source": [
    "The training step returns a dictionary containing the following things:\n",
    "* 'loss': the total loss computed for the batch, which is used for backpropagation\n",
    "* 'model_outputs': containing the three outputs of the forward pass ('pred_energy', 'pred_gradients', 'pred_diff')\n",
    "* 'projected_gradient_difference': the difference between the predicted and true energy gradients projected (to preserve the number of electrons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39622b2",
   "metadata": {},
   "source": [
    "## 4 Running a full training epoch\n",
    "\n",
    "The LitModule uses a specific \"syntax\" to handle these training details under the hood.\n",
    "For instance, the backwards on the loss and also the optimizer step are performed automatically.\n",
    "The lightning model uses for that by default the \"loss\" value\n",
    "returned in the output dictionary of the training step.\n",
    "\n",
    "Furthermore, in the lightning module we do not see an explicit training loop\n",
    "that loops over the batches in the train_loader.\n",
    "This is automatically handled by the pytorch lightning trainer that combines the\n",
    "model with the dataloader(s), e.g. in\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "In addition, the logging to the tensorboard is handled simply via self.log in the lightning module\n",
    "and uses the logger that is attached to the trainer (via the trainer that is attached to the model).\n",
    "\n",
    "Additionally, the lightning module handles the validation loop automatically.\n",
    "It works via the validation_step method similar to the training loop based on the training_step method.\n",
    "There are even quite a bit more methods that follow a standard syntax and can be used to\n",
    "achieve certain behavior during training, e.g. on_epoch_start, on_epoch_end, etc.\n",
    "see https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4fa49",
   "metadata": {},
   "source": [
    "Additional information to the lightning module:\n",
    "\n",
    "1. The **Lightning module** holds our machine learning model at its core and defines what to train and how to train. There are different subclasses implemented in the code:\n",
    "* The **model architetcture** is defined under \"mldft_module.net\". Its forward pass is called in the ```forward``` method of the mldft_module. \n",
    "+ *Analogy:* You can think of ```__init__``` as the ingredient list of a recipe and ```forward``` as a simple instruction. However, the magic happens in between...\n",
    "* The **training logic** lies in the ```training step```.\n",
    "* The **validation and test logic** is in the ```validation_step``` and ```test_step```.\n",
    "* The **optimizer(s)** are set in ```configure_optimizers```.\n",
    "+ *Analogy:* To continue with the analogy, you can think of a the additional functions in the lightning module class as your way to optimize the recipe. With each training step you learn new things and adjuste the recipe (parameters). To make sure your changes are actually good, you also continuously validate it. The optimizatation process happens in this cooking example in your brain as you consider adding more salt etc. In the code, the opimization process happens in the optimizer function.\n",
    "\n",
    "2. The **LightningDataModule** organizes all the data-related logic:\n",
    "* With the ```setup``` function in the DataModule class one defines how to load the data.\n",
    "* Also DataLoaders have to be created with in the ```train_dataloader``` and the ```val_dataloader``` etc.\n",
    "* Optionally, one can define a preprocessing of the data.\n",
    "\n",
    "+ Note: The LightningDataModule helps keeping data handling clean and separate from the model logic.\n",
    "\n",
    "3. Think of the **Trainer** as an orchestrator, which handels:\n",
    "* Training loops\n",
    "  * Note: You don't manualy write the training loops in Lightning - the ```Trainer```automates them.\n",
    "* Validation & testing\n",
    "* Logging\n",
    "* Checkpointing\n",
    "* Device placement (CPU, GPU)\n",
    "* Distributing training \n",
    "\n",
    "Important note: In our file structure, you can find a [\"train.py\"](../../mldft/ml/train.py) file which is the main entry point for training. It instantiates all relevant components, i.e. Trainer, datamodule, lightning module, etc. \n",
    "\n",
    "In a subfolder data, there is a [\"datamodule.py\"](../../mldft/ml/data/datamodule.py) file associated with the DataModule and lastly in the folder models a [\"mldft_module.py\"](../../mldft/ml/models/mldft_module.py) file which handels the core of the model. \n",
    "[Config files](../../configs/ml/train.yaml) (as discussed in [Tutorial 4](./tutorial_4_hydra_omegaconf.ipynb)) are the place where most the variables are stored for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5303a",
   "metadata": {},
   "source": [
    "#### Now it all comes together in our very first training epoch\n",
    "The lightning module follows a specific syntax of methods which will be executed in a very specific order during training. For instance, the `on_train_epoch_start` method will be executed every time when a training epoch is started (as one might have guessed). Similarly, there is the`on_before_backward` method which is called shortly before the backward or the `on_validation_batch_end` that is called after the processing of each validation batch. \n",
    "\n",
    "Below, we will execute a \"full training\" (but only for one epoch) and log all these methods in the order in which they are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# freshly instantiate the trainer for a clean state\n",
    "trainer = instantiate(config.trainer, enable_progress_bar=False)\n",
    "\n",
    "# disable all user warnings for the following trainer.fit call\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "    trainer.fit(mldft_module, datamodule=datamodule);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57bcfa",
   "metadata": {},
   "source": [
    "# Appendix 1: Automatic differentiaton\n",
    "\n",
    "We will have a short intermezzo on understanding how automatic differentiation (via backpropagation) works in PyTorch.\n",
    "When you have a tensor with requires_grad=True, all operations on that tensor are tracked\n",
    "and a computation graph is built in the background.\n",
    "Then when you call backward() on a tensor, the gradients of that tensor with respect to\n",
    "all tensors that have requires_grad=True and were used to compute that tensor\n",
    "are computed via backpropagation through the computation graph.\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3 * x + 1\n",
    "print(\"y:\", y, \"\\n\")\n",
    "y.backward()  # this computes the gradient of y with respect to x via backpropagation\n",
    "print(\"dy/dx:\", x.grad, \"\\n\")  # dy/dx = 2*x + 3 = 2*2 + 3 = 7\n",
    "\n",
    "# small subtlety: if you do multiple operations on a tensor\n",
    "# the gradients are accumulated in the .grad attribute\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y1 = x**2\n",
    "y2 = x**3\n",
    "y1.backward()  # this computes the gradient of y1 with respect to x via backpropagation\n",
    "print(\"dy1/dx:\", x.grad)  # dy1/dx = 2*x = 2*2 = 4\n",
    "y2.backward()  # this computes the gradient of y2 with respect to x via backpropagation\n",
    "print(\"dy1/dx + dy2/dx:\", x.grad, \"\\n\")  # dy1/dx + dy2/dx = 2*x + 3*x**2 = 2*2 + 3*2**2 = 16\n",
    "\n",
    "# to zero the gradients, you can use the zero_() method\n",
    "x.grad.zero_()\n",
    "print(\"zeroed gradients:\", x.grad, \"\\n\")\n",
    "\n",
    "# detach can be used to stop tracking operations on a tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = y.detach() + 3 * x  # detach stops tracking operations on y\n",
    "print(\"z:\", z)\n",
    "z.backward()  # this computes the gradient of z with respect to x via backpropagation\n",
    "print(\"dz/dx:\", x.grad, \"\\n\")  # dz/dx = 3, since y was detached\n",
    "\n",
    "# by default, after one calls backward(), the computation graph is deleted to save memory\n",
    "# if you want to call backward() multiple times on the same graph, for instance to compute a second derivative\n",
    "# (as we actually do in our project when we first compute the energy gradient w.r.t. the density\n",
    "# and then use that energy gradient to compute a loss function that is then used\n",
    "# for another backward call to update the model parameters)\n",
    "# in this case you need to specify retain_graph=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**6\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)[\n",
    "    0\n",
    "]  # this computes dy/dx = 6*x**5\n",
    "print(\"dy/dx:\", dy_dx)\n",
    "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]  # this computes d2y/dx2 = 30*x**4 = 30*2**4 = 480\n",
    "print(\"d2y/dx2:\", d2y_dx2, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af7337",
   "metadata": {},
   "source": [
    "## Appendix 2: Partial\n",
    "\n",
    "Since the optimizer in the config is only partially (\"_partial_\") initialized, we want to take a look at what this actually means in the example below.\n",
    "\n",
    "As you might know the standard normal distribution is a special case of a classical Gaussian distribution. To include this knowledge but simplify futher calling, we could use the partial function and with it specify the necessary mean and standard deviation properties that make a Gaussian a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def gaussian(x, mean, std):\n",
    "    return torch.exp(-0.5 * ((x - mean) / std) ** 2) / (std * (2 * torch.pi) ** 0.5)\n",
    "\n",
    "\n",
    "standard_normal = partial(gaussian, mean=0.0, std=1.0)\n",
    "print(standard_normal)\n",
    "# standard_normal is now a function that only takes x as argument\n",
    "# and mean and std are fixed to 0.0 and 1.0 respectively\n",
    "x = torch.linspace(-5, 5, steps=100)\n",
    "y = standard_normal(x=x)\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "plt.title(\"Standard normal distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a4ad4",
   "metadata": {},
   "source": [
    "## Appendix 3: Optimizer\n",
    "\n",
    "Now, we want to examplify how the updating of the model parameters works during training\n",
    "for that we need to attach an optimizer to the model. \n",
    "\n",
    "Next, if you look carefully you will find that the optimizer in the config is only partially (\"_partial_\") initialized. This means that some of the arguments are missing and will be filled in later (more info see Appendix 1). In particular the model parameters that should be optimized are missing, because the model parameters are not known before the model is instantiated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_partially_initialized = instantiate(config.model.optimizer)\n",
    "optimizer = optimizer_partially_initialized(params=mldft_module.parameters())\n",
    "\n",
    "# an alternative more compact option would be the following:\n",
    "# optimizer = instantiate(config.model.optimizer, params=mldft_module.parameters())\n",
    "\n",
    "mldft_module.optimizer = optimizer  # add the optimizer to the module\n",
    "print(\"Successfully instantiated optimizer and linked it with model parameters:\", type(optimizer))\n",
    "\n",
    "for name, model_param in mldft_module.named_parameters():\n",
    "    print(name, model_param.shape)\n",
    "    break  # just the first parameter\n",
    "\n",
    "# first we zero the gradients of the model parameters\n",
    "mldft_module.optimizer.zero_grad()\n",
    "# print the gradient of the first parameter (should be None after zeroing the grads):\n",
    "print(\"Gradient of first parameter before backward:\", model_param.grad)\n",
    "# then we call backward on the loss to compute the gradients of the model parameters\n",
    "try:\n",
    "    train_step_out[\"loss\"].backward(\n",
    "        retain_graph=False\n",
    "    )  # so that this cell can in principle be run multiple times\n",
    "    # now the gradients of the model parameters are stored in the .grad attribute of each parameter\n",
    "    print(\"Gradient of first parameter after backward:\", model_param.grad, model_param.grad.shape)\n",
    "    old_model_param = (\n",
    "        model_param.clone().detach()\n",
    "    )  # clone and detach to keep a copy of the old parameters\n",
    "\n",
    "    # now, we can update the model parameters with one step of the optimizer\n",
    "    mldft_module.optimizer.step()\n",
    "    print(\n",
    "        \"Maximum relative change in first parameter after one optimizer step:\",\n",
    "        ((model_param - old_model_param) / old_model_param).abs().max(),\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(\"Caught expected RuntimeError due to multiple backward calls on the same graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d8b11",
   "metadata": {},
   "source": [
    "# Appendix 4: Dataset statistics\n",
    "\n",
    "One small but not to be underestimated detail of our training are the dataset statistics.\n",
    "These are used to standardize the input densities and the output energy labels\n",
    "to improve and stabilize training.\n",
    "As such, the dataset statistics are specific to which dataset (QM9 or QMUGS) and energy label is used (E_kin, E_xc, E_kin + E_xc, etc.),\n",
    "as well as to which transforms are applied to the input densities (e.g. local_frames_global_symmetric_natrep).\n",
    "\n",
    "The dataset_statistics are essentially a .zarr folder, which can be seen in the config path. After instantiating it, we see for each relevant quantity some additonal statistical values, like the mean and std, as well as the abs_max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb328f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the respective part in the config to verify that:\n",
    "rich.print(dict_to_tree(config.data.dataset_statistics, guide_style=\"dim\"))\n",
    "\n",
    "from mldft.ml.preprocess.dataset_statistics import DatasetStatistics\n",
    "\n",
    "dataset_statistics = instantiate(config.data.dataset_statistics)\n",
    "print(\"Successfully instantiated dataset_statistics:\", type(dataset_statistics))\n",
    "dataset_statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
