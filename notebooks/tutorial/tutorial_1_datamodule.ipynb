{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1be328",
   "metadata": {},
   "source": [
    "# Tutorial 1: Understanding our OFDatamodule that handles all data samples during training\n",
    "\n",
    "The goal of this tutorial is to understand the behaviour and interplay of following classes that handle the loading and processing of our data: OFDataset, OFData, OFBatch, OFLoader, OFDataModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rich\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# this makes sure that code changes are reflected without restarting the notebook\n",
    "# this can be helpful if you want to play around with the code in the repo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# omegaconf is used for configuration management\n",
    "# omegaconf custom resolvers are small functions used in the config files like \"get_len\" to get lengths of lists\n",
    "from mldft.utils import omegaconf_resolvers  # this registers omegaconf custom resolvers\n",
    "from mldft.utils.log_utils.config_in_tensorboard import dict_to_tree\n",
    "\n",
    "# download a small dataset from huggingface that contains QM9 and QMugs data (possibly already downloaded)\n",
    "# and change the DFT_DATA environment variable to the directory where the data is stored\n",
    "\n",
    "# https://huggingface.co/docs/datasets/cache#cache-directory\n",
    "# The default cache directory is `~/.cache/huggingface/datasets`\n",
    "# You can change it by setting this variable to any path you like\n",
    "CACHE_DIR = None  # e.g. change it to \"./hf_cache\"\n",
    "\n",
    "# clone the full repo\n",
    "# https://huggingface.co/sciai-lab/structures25/tree/main\n",
    "os.environ[\n",
    "    \"HF_HUB_DISABLE_PROGRESS_BARS\"\n",
    "] = \"1\"  # to avoid problems with the progress bar in some environments\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "data_path = snapshot_download(\n",
    "    repo_id=\"sciai-lab/minimal_data_QM9_QMugs\", cache_dir=CACHE_DIR, repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "dft_data = os.environ.get(\"DFT_DATA\", None)\n",
    "os.environ[\"DFT_DATA\"] = data_path\n",
    "print(\n",
    "    f\"Environment variable DFT_DATA has been changed from {dft_data} to {os.environ['DFT_DATA']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d41ea1b",
   "metadata": {},
   "source": [
    "## 1 Loading the datamodule from our gigantic config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac0051",
   "metadata": {},
   "source": [
    "First, we load a large config as Omegaconf Dict config for training a model\n",
    "with the defaut settings for data, optimizer, transforms, basis set, etc.\n",
    "For now you can think of the config as a large nested dictionary that contains all settings\n",
    "and hyperparameters used for training our OF-DFT model.\n",
    "Later in the tutorial ([tutorial_4_hydra_omega_conf](./tutorial_4_hydra_omegaconf.ipynb)), we will go into more detail about how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "# the following initialize already handles the communication and combination\n",
    "# of the different config files, e.g. for data and the model\n",
    "with initialize(version_base=None, config_path=\"../../configs/ml\"):\n",
    "    config = compose(\n",
    "        config_name=\"train.yaml\",\n",
    "    )\n",
    "\n",
    "# remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config.paths.output_dir = \"example_path\"\n",
    "\n",
    "# let us take a look at the part of the config data is used specifically for configuring the data module\n",
    "rich.print(dict_to_tree(config.data.datamodule, guide_style=\"dim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4203c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.datamodule import OFDataModule\n",
    "\n",
    "# we will now use this part of the config to instantiate important individual parts\n",
    "# of the full training pipeline e.g. the datamodule\n",
    "datamodule = instantiate(config.data.datamodule)\n",
    "datamodule.batch_size = 4  # set batch size to 4 (relatively small) for demonstration purposes\n",
    "print(\"Successfully instantiated datamodule:\", type(datamodule))\n",
    "datamodule.setup(stage=\"fit\")  # prepare the data, e.g. split into train, val, test\n",
    "# with stage=\"fit\" no test set is prepared, only the train and validation set used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get a quick look of what is combined in the datamodule, we can look at its __dict__\n",
    "datamodule.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15aa52a",
   "metadata": {},
   "source": [
    "## 2 A first look at the dataset and a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ad939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some place like this import the respective class so that you can click to definition\n",
    "from mldft.ml.data.components.dataset import OFDataset\n",
    "\n",
    "# let's look at the dataset(s):\n",
    "# print the length ot the train and validation set used during training:\n",
    "# so-called \"split files\" are handling the split into disjoint train, val and test set\n",
    "print(f\"Length of train set: {len(datamodule.train_set)}\")\n",
    "print(f\"Length of val set: {len(datamodule.val_set)}\")\n",
    "print(f\"type of train set: {type(datamodule.train_set)}\")\n",
    "print(f\"isinstance of OFDataset {isinstance(datamodule.train_set, OFDataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.of_data import OFData\n",
    "\n",
    "# get a single sample for the train set\n",
    "sample = datamodule.train_set[0]\n",
    "print(\"Atom positions:\", sample.pos)\n",
    "print(\"Atom types:\", sample.atomic_numbers)\n",
    "print(\"number of coefficients:\", sample.coeffs.shape)\n",
    "print(\n",
    "    \"Integrals of basis functions used to describe the density:\", sample.dual_basis_integrals.shape\n",
    ")\n",
    "print(\"scf_iteration:\", sample.scf_iteration)\n",
    "print(\"Energy label (kinetic energy + XC energy):\", sample.energy_label)\n",
    "print(\"Energy key:\", datamodule.train_set.of_data_kwargs[\"energy_key\"])\n",
    "print(\"Is sample an instance of OFData?\", isinstance(sample, OFData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e42ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.utils.molecules import build_molecule_ofdata\n",
    "\n",
    "# need basis info to build a pySCF molecule object\n",
    "# see below for more details on basis_info\n",
    "basis_info = instantiate(config.data.basis_info)\n",
    "\n",
    "# build a pySCF molecule object from the OFData sample\n",
    "mol = build_molecule_ofdata(sample, basis=basis_info.basis_dict)\n",
    "print(f\"type : {type(mol)}, xyz string of that molecule:\\n\")\n",
    "print(mol.tostring(\"xyz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyscf.lib import param\n",
    "\n",
    "bohr2ang = param.BOHR  # approx 0.529177 , i.e. 1 bohr = 0.529177 Angstrom\n",
    "# our dataset works in the \"distance\" unit Bohr but others (like RDKit in this case) work in Angstrom\n",
    "# to see how both are consistent we can convert the positions\n",
    "print(\"Positions in Angstrom:\\n\", sample.pos * bohr2ang)\n",
    "\n",
    "# note that from the pyscf.Mole object we can also get the atom positions in different units via:\n",
    "print(\"Positions in Angstrom from pyscf.Mole:\\n\", mol.atom_coords(unit=\"Angstrom\"))\n",
    "print(\"Positions in Bohr from pyscf.Mole:\\n\", mol.atom_coords(unit=\"Bohr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6261df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "\n",
    "from mldft.utils.conversions import pyscf_to_rdkit\n",
    "\n",
    "# please, note that the transformation from a set of atom positions (e.g. xyzfile) to an rdkit molecule\n",
    "# with bonds (and nice pictures/structure as below) is not necessarily well defined,\n",
    "# since it is non-trivial to infer chemical bonds from just positions and atom types\n",
    "# (though this should not be an issue for classic QM9 and QMUGS molecules)\n",
    "\n",
    "rdkit_mol = pyscf_to_rdkit(mol)\n",
    "print(\"type\", type(rdkit_mol))\n",
    "# show the molecule with rdkit\n",
    "img = Draw.MolToImage(rdkit_mol)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7cf40",
   "metadata": {},
   "source": [
    "## 3 Our density representation and the BasisInfo class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123d26e",
   "metadata": {},
   "source": [
    "We represent the electron density $\\rho(\\vec r)$, which is a function of 3D space, as a linear combination of so-called atom-cendered basis functions (each is a function of 3D space localized around a different atoms in the molecule).\n",
    "$$\\rho(\\vec r) = \\sum_\\mu p_\\mu \\omega_\\mu(\\vec r)$$\n",
    "$p_\\mu$ are the density coefficients and $\\omega_\\mu(\\vec r)$ are the different basis functions. We use Gaussian type orbitals (GTOs) as basis functions which combine a Gaussian-like radial part with a spherical harmonic angular part. Please take a look at the [STRUCTURES25 paper](https://pubs.acs.org/doi/10.1021/jacs.5c06219) for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80f53b",
   "metadata": {},
   "source": [
    "Above, we have seen a visulization of a electron density using the coefficients and the basis functions.\n",
    "let us now look at the basis info object in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed221186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.basis_info import BasisInfo\n",
    "\n",
    "# the essential info about all basis functions for the different atom types is stored in\n",
    "# basis_info.basis_dict, a dictionary with the following structure:\n",
    "# key: atom type val: list of (angular momentum, [exponent, weighting coeffs for contractions])\n",
    "# see https://pyscf.org/user/gto.html#basis-format for details\n",
    "basis_info.basis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"atomic numbers in the dataset:\", basis_info.atomic_numbers)\n",
    "print(\"Number of basis functions/coeffs per atom type:\", basis_info.basis_dim_per_atom)\n",
    "\n",
    "# for instance, we can take a look at the integrals of the basis functions for Hydrogen:\n",
    "# all basis functions that have l>0 integrate to zero:\n",
    "basis_info.integrals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089e111",
   "metadata": {},
   "source": [
    "## 4 Our dataloader converts OFData into OFBatch objects\n",
    "We are gradually moving towards training a model. For that, we take a look at the dataloaders that combine multiple molecules into batches, which are then passed to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.loader import OFLoader\n",
    "from mldft.ml.data.components.of_batch import OFBatch\n",
    "\n",
    "datamodule.batch_size = 4  # set batch size to 4 (relatively small) for demonstration purposes\n",
    "train_loader = datamodule.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    # get the first batch in the train loader as the model would\n",
    "    batch\n",
    "    break\n",
    "\n",
    "# an alternative to get the first batch from the train_loader is the following:\n",
    "# batch = next(iter(train_loader))\n",
    "\n",
    "# one special thing about geometric graph data:\n",
    "# different molecules have different number of atoms, therefore combining them into\n",
    "# one batch is not as simple as stacking them into a tensor\n",
    "# but it is more an appending into one large graph with all atoms and the\n",
    "# batch.batch tensor indicating which atom belongs to which molecule in the large graph\n",
    "print(\"number of molecules in the batch:\", batch.num_graphs)\n",
    "print(\"Number of atoms in the batch:\", batch.num_nodes)\n",
    "print(\"batch.batch:\", batch.batch, \"len(batch.batch):\", len(batch.batch))\n",
    "print(\"Length of 'concatenated' atom positions:\", batch.pos.shape)\n",
    "print(\"Length of 'concatenated' atomic numbers:\", batch.atomic_numbers.shape)\n",
    "\n",
    "# find out how many atoms are in each molecule in the batch\n",
    "num_atoms_per_mol = torch.bincount(batch.batch)\n",
    "print(\"Number of atoms per molecule in the batch:\", num_atoms_per_mol)\n",
    "# average, max, min, number of atoms in the molecules in the batch\n",
    "print(\n",
    "    \"average number of atoms per molecule in the batch:\", num_atoms_per_mol.float().mean().item()\n",
    ")\n",
    "print(\"max number of atoms per molecule in the batch:\", num_atoms_per_mol.max().item())\n",
    "print(\"min number of atoms per molecule in the batch:\", num_atoms_per_mol.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch can be separated into individual data samples (molecules) again via:\n",
    "list_of_molecules = batch.to_data_list()\n",
    "print(\n",
    "    \"Length of list_of_molecules:\",\n",
    "    len(list_of_molecules),\n",
    "    \"first mol in list is:\",\n",
    "    list_of_molecules[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e47e1",
   "metadata": {},
   "source": [
    "It is also possible to create batches manually from a list of OFData samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79531e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mldft.ml.data.components.of_data import Representation\n",
    "\n",
    "# let us add a new property to each sample\n",
    "# (this is a bit special since we always specify the representation of items, see sample.representations)\n",
    "for molecule in list_of_molecules:\n",
    "    molecule.add_item(\n",
    "        key=\"example_property\", value=torch.tensor(42.0), representation=Representation.SCALAR\n",
    "    )\n",
    "\n",
    "batch = OFBatch.from_data_list(list_of_molecules)\n",
    "batch.example_property"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
