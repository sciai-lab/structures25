{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1be328",
   "metadata": {},
   "source": [
    "# Tutorial 4: Hydra, OmegaConf, Overrides\n",
    "\n",
    "In this tutorial, you will learn more about the underlying structure which includes config files, the hydra structure and the OmegaConf magic. We will also cover the topic of overrides which allows us to access the none-default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0e451",
   "metadata": {},
   "source": [
    "## 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# this makes sure that code changes are reflected without restarting the notebook\n",
    "# this can be helpful if you want to play around with the code in the repo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# omegaconf is used for configuration management\n",
    "# omegaconf custom resolvers are small functions used in the config files. For example, \"get_len\" is used to get lengths of lists.\n",
    "from mldft.utils import omegaconf_resolvers  # this registers omegaconf custom resolvers\n",
    "\n",
    "# download a small dataset from huggingface that contains QM9 and QMugs data (possibly already downloaded)\n",
    "# and change the DFT_DATA environment variable to the directory where the data is stored\n",
    "\n",
    "# https://huggingface.co/docs/datasets/cache#cache-directory\n",
    "# The default cache directory is `~/.cache/huggingface/datasets`\n",
    "# You can change it by setting this variable to any path you like\n",
    "CACHE_DIR = None  # e.g. change it to \"./hf_cache\"\n",
    "\n",
    "# clone the full repo\n",
    "# https://huggingface.co/sciai-lab/structures25/tree/main\n",
    "os.environ[\n",
    "    \"HF_HUB_DISABLE_PROGRESS_BARS\"\n",
    "] = \"1\"  # to avoid problems with the progress bar in some environments\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "data_path = snapshot_download(\n",
    "    repo_id=\"sciai-lab/minimal_data_QM9_QMugs\", cache_dir=CACHE_DIR, repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "dft_data = os.environ.get(\"DFT_DATA\", None)\n",
    "os.environ[\"DFT_DATA\"] = data_path\n",
    "print(\n",
    "    f\"Environment variable DFT_DATA has been changed from {dft_data} to {os.environ['DFT_DATA']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e2fca",
   "metadata": {},
   "source": [
    "# 1 Hydra\n",
    "\n",
    "Hydra is used for configuration management. It can be thought of as a tree of configuration files. \n",
    "Usually a parent config file is used to set global variables and to specify which other child config files to use. \n",
    "The child config files then set specific variables that are usually related to a specific topic (e.g. model architecture, training parameters, data parameters, etc.).\n",
    "\n",
    "To understand how the child config files are impemented, it is recommended to take a look a the OmegaConf magic in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e12598",
   "metadata": {},
   "source": [
    "# 2 OmegaConf Magic\n",
    "\n",
    "First, we look at and example config and see how the tree structure looks like. \n",
    "\n",
    "Taking a closer look at the config file, we can see that a OmegaConf resolver is used to get the length of the list of hidden layers.\n",
    "In an additional example in the Apendix 1, we show how you can create your own custom resolvers.\n",
    "\n",
    "\n",
    "We will use this config later to instantiate a model and a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd83bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# understanding the omegaconf config magic and syntax:\n",
    "example_config = {\n",
    "    \"sub_dict\": {\"a\": 1, \"b\": 2, \"l\": [1, 2, 3]},\n",
    "    \"len_of_l\": \"${get_len:${sub_dict.l}}\",  # this uses the custom resolver \"get_len\" to get the length of list l\n",
    "    # use this structure to cross-reference within a config ${sub_dict.l}\n",
    "    \"mlp\": {\n",
    "        \"_target_\": \"mldft.ml.models.components.mlp.MLP\",  # this is used by hydra to instantiate an object of the given class\n",
    "        \"in_channels\": 3,\n",
    "        \"hidden_channels\": [16, 16, 1],\n",
    "    },\n",
    "}\n",
    "\n",
    "omegaconf_example_config = OmegaConf.create(example_config)\n",
    "# if you print the config naively, it shows just the strings\n",
    "print(\"OmegaConf config:\", omegaconf_example_config)\n",
    "# BUT if you access the value, it resolves the string using the custom resolver\n",
    "print(\"Value from accessing len_of_l\", omegaconf_example_config.len_of_l)  # prints 3\n",
    "\n",
    "# instantiate the MLP based on the example  config above:\n",
    "# when calling instantiate, the _target_ field is used to find the class\n",
    "# and all other fields are passed as arguments to the class constructor:\n",
    "# in this case the MLP class from mldft.ml.models.components.mlp\n",
    "# with in_channels=3 and hidden_channels=[16, 16, 1] as arguments\n",
    "mlp = instantiate(omegaconf_example_config.mlp)\n",
    "print(\"\\nInstantiated MLP:\", mlp, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba731ac",
   "metadata": {},
   "source": [
    "In yaml formatting the above config will look something like:\n",
    "\n",
    "```\n",
    "# example_config.yaml\n",
    "\n",
    "sub_dict:\n",
    "  a: 1\n",
    "  b: 2\n",
    "  l: [1, 2, 3]\n",
    "\n",
    "# uses the custom resolver \"get_len\" to get the length of list l\n",
    "len_of_l: ${get_len:${sub_dict.l}}\n",
    "\n",
    "# use this structure to cross-reference within a config: ${sub_dict.l}\n",
    "mlp:\n",
    "  # used by Hydra to instantiate an object of the given class\n",
    "  _target_: mldft.ml.models.components.mlp.MLP\n",
    "  in_channels: 3\n",
    "  hidden_channels: [16, 16, 1]\n",
    "```\n",
    "\n",
    "All our configs (in hierarchical structure are collected in the `configs` folder). The highest level config for model training to start from is the [configs/ml/train.yaml](../../configs/ml/train.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928e93a",
   "metadata": {},
   "source": [
    "## 3 Config for model training\n",
    "\n",
    "Now, we want to load the config for the actual model training. Additionally, we load the data and create batches which can easierly be  handled by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "# load the config as Omegaconf Dict config for training a model\n",
    "# with the defaut settings for data, optimizer, transforms, basis set, etc.\n",
    "# this already handles the communication and combination of the different config files, e.g. for data and the model\n",
    "with initialize(version_base=None, config_path=\"../../configs/ml\"):\n",
    "    config = compose(\n",
    "        config_name=\"train.yaml\",\n",
    "        overrides=[\n",
    "            \"data.dataset_name=QM9_perturbed_fock\",  # this will no longer be necessary once the \"fixed\" is removed from the dataset_name\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config.paths.output_dir = \"example_path\"\n",
    "\n",
    "datamodule = instantiate(config.data.datamodule)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "datamodule.batch_size = 4  # set batch size to 4 (relatively small) for demonstration purposes\n",
    "train_loader = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec87358",
   "metadata": {},
   "source": [
    "## 4 Overriding the default config\n",
    "\n",
    " As we not always want to use the default config, here is an examples of how to override settings. \n",
    "In more detail, we now wish to use the QMugs dataset instead of the default QM9 dataset.\n",
    "\n",
    "Below, we also prepare the dataset for the fit stage of training, i.e. we use a smaller subset of the data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../../configs/ml\"):\n",
    "    config_qmugs = compose(\n",
    "        config_name=\"train.yaml\",\n",
    "        overrides=[\n",
    "            # this overrides the data used to the qmugs dataset\n",
    "            \"data.dataset_name=QMUGS_perturbed_fock\",  # with the dot we override a nested field\n",
    "            \"data/transforms=no_basis_transforms\",  # with the / we override a whole file\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# remove the hydra specific stuff that only works in @hydra.main decorated functions\n",
    "config_qmugs.paths.output_dir = \"example_path\"\n",
    "\n",
    "datamodule_qmugs = instantiate(config_qmugs.data.datamodule)\n",
    "datamodule_qmugs.setup(stage=\"fit\")  # prepare the datasets\n",
    "print(f\"Length of qmugs train set: {len(datamodule_qmugs.train_set)}\")\n",
    "print(f\"Length of qmugs val set: {len(datamodule_qmugs.val_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c593fa7",
   "metadata": {},
   "source": [
    "To get a better intuition on how the QMugs datset is different from QM9 in terms of complexity, we visualize a QMugs molecule below. For visualizations of example QM9 molecules, please have a look at [Tutorial 2](tutorial_2_visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# keep only the program name so downstream parsers don't see Jupyter's -f=...\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "import pyvista\n",
    "\n",
    "from mldft.utils.molecules import build_molecule_ofdata\n",
    "from mldft.utils.visualize_3d import get_sticks_mesh_dict\n",
    "\n",
    "basis_info_qmugs = instantiate(config_qmugs.data.basis_info)\n",
    "\n",
    "# look at a qmugs molecule to see that they are larger than qm9 molecules\n",
    "for sample_qmugs in datamodule_qmugs.train_set:\n",
    "    if sample_qmugs.mol_id.startswith(\"qmugs\"):\n",
    "        print(\"Found a qmugs molecule:\", sample_qmugs.mol_id)\n",
    "        break\n",
    "\n",
    "mol_qmugs = build_molecule_ofdata(sample_qmugs, basis=basis_info_qmugs.basis_dict)\n",
    "\n",
    "\n",
    "# this give a ball and stick model of the molecule\n",
    "molecule_mesh = get_sticks_mesh_dict(mol_qmugs)\n",
    "molecule_mesh[\"opacity\"] = 1\n",
    "\n",
    "# plot the molecule and the global frame using pyvista:\n",
    "pyvista.set_jupyter_backend(\"html\")\n",
    "pl = pyvista.Plotter(off_screen=True, notebook=True, image_scale=1)\n",
    "pl.camera_position = \"zx\"\n",
    "pl.enable_parallel_projection()\n",
    "pl.add_mesh(**molecule_mesh)\n",
    "pl.enable_shadows()\n",
    "pl.reset_camera(\n",
    "    bounds=0.9\n",
    "    * np.stack([mol_qmugs.atom_coords().min(0), mol_qmugs.atom_coords().max(0)], axis=1).flatten()\n",
    ")\n",
    "\n",
    "img = pl.show(screenshot=True, window_size=(800, 400))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4deb2",
   "metadata": {},
   "source": [
    "## Appendix 1: Self-created custom resolver\n",
    "\n",
    "Above, get_len is already defined and registered as custom resolver in mldft.utils.omegaconf_resolvers.\n",
    "But, we can also define our own omega conf custom resolver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016019c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if \"sum is already registered\"\n",
    "if OmegaConf.has_resolver(\"sum\"):\n",
    "    print(\"sum is already registered\")\n",
    "else:\n",
    "    print(\"registering sum\")\n",
    "    # register a custom resolver \"sum\" that sums up all its arguments\n",
    "    OmegaConf.register_new_resolver(\"sum\", lambda *args: sum(args))\n",
    "\n",
    "example_config = {\n",
    "    \"sub_dict\": {\"a\": 17, \"b\": -5},\n",
    "    \"sum_a_b\": \"${sum:${sub_dict.a}, ${sub_dict.b}}\",  # this uses the custom resolver \"get_len\" to get the length of list l\n",
    "    # use this structure to cross-reference within a config ${sub_dict.l}\n",
    "}\n",
    "\n",
    "omegaconf_example_config = OmegaConf.create(example_config)\n",
    "# if you print the config naively, it shows just the strings\n",
    "print(\"OmegaConf config:\", omegaconf_example_config)\n",
    "# BUT if you access the value, it resolves the string using the custom resolver\n",
    "print(\"Value from accessing len_of_l\", omegaconf_example_config.sum_a_b)  # prints 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
